(window.webpackJsonp=window.webpackJsonp||[]).push([[0],{100:function(e,t,a){e.exports=a.p+"static/media/state.a8ac3032.png"},101:function(e,t,a){e.exports=a.p+"static/media/RR.d9f00e2e.png"},102:function(e,t,a){e.exports=a.p+"static/media/rewards.64e75652.png"},103:function(e,t,a){e.exports=a.p+"static/media/dots.f1d9cdc5.png"},104:function(e,t){e.exports="data:image/png;base64,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"},105:function(e,t,a){e.exports=a.p+"static/media/results1.d8b47c61.png"},106:function(e,t,a){e.exports=a(227)},111:function(e,t,a){},226:function(e,t,a){},227:function(e,t,a){"use strict";a.r(t);var n=a(0),i=a.n(n),r=a(32),o=a.n(r),l=(a(111),a(1)),c=a.n(l),s=a(12),m=a(13),u=a(16),p=a(14),d=a(45),g=a(15),h=a(88),f=a(84),y=a.n(f),E=a(86),b=a.n(E),w=a(28),j=a.n(w),x=a(18),v=a.n(x),A=(a(35),a(29)),T=function(e){function t(){return Object(s.a)(this,t),Object(u.a)(this,Object(p.a)(t).apply(this,arguments))}return Object(g.a)(t,e),Object(m.a)(t,[{key:"render",value:function(){var e=this,t=this.props.classes;return i.a.createElement("div",{className:t.root},i.a.createElement(y.a,{position:"fixed",style:{backgroundColor:"white",color:"#4885ed",boxShadow:"none"}},i.a.createElement(b.a,null,i.a.createElement(j.a,{variant:"h6",color:"inherit",className:t.grow,onClick:function(){return e.props.history.push("/")}},"Andrea Cortoni"),i.a.createElement(v.a,{color:"inherit",onClick:function(){return e.props.history.push("/about")}},"About"),i.a.createElement(v.a,{color:"inherit",onClick:function(){return e.props.history.push("/resume")}},"Resume"),i.a.createElement(v.a,{color:"inherit",onClick:function(){return e.props.history.push("/projects")}},"Projects"),i.a.createElement(v.a,{color:"inherit",style:{marginRight:200}},"Contact"))))}}]),t}(i.a.Component),k=Object(A.d)(Object(h.withStyles)({root:{flexGrow:1},grow:{flexGrow:1,marginLeft:200,cursor:"pointer"},menuButton:{marginLeft:-12,marginRight:20}})(T)),S=a(89),P=a.n(S),C=a(90),O=a.n(C),L=a(3),R=a.n(L),D=a(91),I=a.n(D),F=a(46),B=a.n(F),N=(a(81),[{url:"https://github.com/cortandr",className:"fa-github"},{url:"https://gitlab.com/andrewgbliss",className:"fa-linkedin"},{url:"https://www.npmjs.com/~andrewgbliss",className:"fa-twitter"}]),z=function(e){function t(e){var a;return Object(s.a)(this,t),(a=Object(u.a)(this,Object(p.a)(t).call(this,e))).state={isHide:!1},a.scrollTo=a.scrollTo.bind(Object(d.a)(a)),a}return Object(g.a)(t,e),Object(m.a)(t,[{key:"scrollTo",value:function(){O()(this.Work,{offset:-50,align:"top",duration:1500})}},{key:"render",value:function(){return i.a.createElement(c.a,{container:!0,justify:"center",style:{overflowY:"hidden"}},i.a.createElement(k,null),i.a.createElement("div",{style:{width:"100%",height:"100vh",top:0,left:0,position:"absolute",overflowY:"hidden"}},i.a.createElement(I.a,{params:{retina_detect:!0,particles:{line_linked:{enable:!0,distance:150,color:"#4885ed",opacity:.4,width:1},number:{value:70},color:{value:"#4885ed"},size:{value:3}}}})),i.a.createElement(c.a,{container:!0,justify:"center",style:{height:"100vh",verticalAlign:"middle",position:"relative"}},i.a.createElement(c.a,{container:!0,justify:"center"},i.a.createElement(c.a,{item:!0,xs:5},i.a.createElement(c.a,{container:!0,alignItems:"vertical",style:{marginTop:260}},i.a.createElement(c.a,{item:!0,style:{height:70}},i.a.createElement(R.a,{in:!0,timeout:1e3},i.a.createElement("h1",{style:{color:"#4885ed",fontSize:40}},"Ciao, I'm Andrea."))),i.a.createElement(R.a,{in:!0,timeout:3e3},i.a.createElement(c.a,{item:!0},i.a.createElement("p",{style:{fontSize:20}},"I'm a Software Engineer with a strong passion for Machine Learning and its incredible potential."),i.a.createElement("p",{style:{fontSize:15}},"Former Software Engineering Intern at",i.a.createElement("span",{style:{fontSize:20,color:"#cc0000"}}," TESLA "),",",i.a.createElement("span",{style:{fontSize:20,color:"#006699"}}," IBM"),", embedded software developer at",i.a.createElement("span",{style:{fontSize:20,color:"#8FD1F7"}}," ISMB"),".")))),i.a.createElement(R.a,{in:!0,timeout:3e3},i.a.createElement("div",null,i.a.createElement(B.a,{icons:N,iconSize:"1.3em",iconColor:"#A9A9A9"})))),i.a.createElement(c.a,{item:!0,xs:3},i.a.createElement(R.a,{in:!0,timeout:1e3},i.a.createElement("img",{src:P.a,alt:"pic",height:300,width:300,style:{borderRadius:"50%",marginTop:250}})))),i.a.createElement(c.a,{container:!0,justify:"center",style:{height:10}},i.a.createElement(c.a,{item:!0,xs:2,style:{textAlign:"center",color:"lightgray",marginTop:100}},"\xa9Andrea Cortoni"))))}}]),t}(i.a.Component),M=a(27),V=a(38),X=a.n(V),q=[{url:"https://github.com/andrewgbliss",className:"fa-github"},{url:"https://gitlab.com/andrewgbliss",className:"fa-linkedin"},{url:"https://www.npmjs.com/~andrewgbliss",className:"fa-twitter"}],J=function(e){function t(e){var a;return Object(s.a)(this,t),(a=Object(u.a)(this,Object(p.a)(t).call(this,e))).state={work_name:""},a}return Object(g.a)(t,e),Object(m.a)(t,[{key:"render",value:function(){var e=new Date,t=e.toLocaleString("en-us",{month:"long"}),a=e.getDate(),n=e.getFullYear();return i.a.createElement(c.a,{container:!0,justify:"center",style:{backgroundColor:"#F8F8F8",height:250,marginTop:50}},i.a.createElement(c.a,{item:!0,xs:6},i.a.createElement("p",{className:"contact"},"If you want to get in touch send an email to",i.a.createElement("span",{className:"email"}," ",i.a.createElement("a",{href:"mailto:cortandr95@gmail.com"},"cortandr95@gmail.com")))),i.a.createElement(c.a,{item:!0,xs:6},i.a.createElement("p",{className:"update"},"Last updated on ",t," ",a,"th ",n),i.a.createElement(B.a,{icons:q,iconSize:"1.3em",iconColor:"#A9A9A9"})))}}]),t}(i.a.Component),Q=a(92),W=a.n(Q),Y=[{date:"Sept '18 - March '19",title:"TESLA - Software Engineering Intern",skills:["Python","React","Django","Keras","SciKit-Learn","MongoDB"],description:"Swat Team intern in Remanufacturing Department. Mainly developed systems for anomaly detection. Worked on multiple projects involving the application of technologies including Convolutional Neural Networks, Automated Machine Learning, Full stack development, Data analysis."},{date:"July '18 - Sept '18",title:"IBM - Software Engineering Intern",skills:["Python","React","Flask","NLTK","SciKit-Learn","MongoDB"],description:"Worked on bringing automation in talent acquisition processes. Applied data science techniques as well as built RESTful APIs to automate different processes in human resources."},{date:"March '17 - June '17",title:"ISMB - Embedded Software Developer Intern",skills:["C","C++","UWB"],description:"Worked in a team developing technologies for IoT. Extended the firmware of a rover system for supporting the use of UWB module for indoor localization, instead of GPS module."}],K=[{date:"2017-2019",title:"Master of Science in Computer Science, Specialization in AI",description:"KU Leuven, Leuven, Belgium."},{date:"2014-2017",title:"Bachelor of Science in Electronic Engineering",description:"Polytechnic of Turin, Turin, Italy."},{date:"2015-2016",title:"Bachelor of Science in Information Technology Engineering",description:"Tongji University, Shanghai, China."}],H=function(e){function t(){return Object(s.a)(this,t),Object(u.a)(this,Object(p.a)(t).apply(this,arguments))}return Object(g.a)(t,e),Object(m.a)(t,[{key:"render",value:function(){return i.a.createElement(c.a,{container:!0,justify:"center"},i.a.createElement(k,null),i.a.createElement(R.a,{in:!0,timeout:1e3},i.a.createElement(c.a,{container:!0,justify:"center",style:{marginTop:80}},i.a.createElement(c.a,{item:!0,xs:12},i.a.createElement("h1",{style:{color:"#4885ed",textAlign:"center"}},"Experience")))),Y.map(function(e,t){return i.a.createElement(R.a,{in:!0,timeout:1e3*(t+1)},i.a.createElement(c.a,{container:!0,justify:"center"},i.a.createElement(c.a,{item:!0,xs:6},i.a.createElement("p",{style:{textAlign:"right",marginTop:60,marginRight:16}},e.date)),i.a.createElement(c.a,{item:!0,xs:6},i.a.createElement("h3",{style:{color:"#4885ed"}},e.title),i.a.createElement("p",{style:{width:500,textAlign:"justify"}},e.description),i.a.createElement("p",null,e.skills.map(function(e,t){return i.a.createElement(X.a,{label:e,style:{marginRight:5,height:25,borderColor:"#4885ed",color:"#4885ed"},variant:"outlined"})})))))}),i.a.createElement(R.a,{in:!0,timeout:4e3},i.a.createElement(c.a,{container:!0,justify:"center",style:{marginTop:50}},i.a.createElement(c.a,{item:!0,xs:12},i.a.createElement("h1",{style:{color:"#4885ed",textAlign:"center"}},"Education")))),K.map(function(e,t){return i.a.createElement(R.a,{in:!0,timeout:1e3*(t+1)},i.a.createElement(c.a,{container:!0,justify:"center"},i.a.createElement(c.a,{item:!0,xs:6},i.a.createElement("p",{style:{textAlign:"right",marginTop:40,marginRight:16}},e.date)),i.a.createElement(c.a,{item:!0,xs:6},i.a.createElement("h3",{style:{color:"#4885ed"}},e.title),i.a.createElement("p",{style:{width:500,textAlign:"justify"}},e.description))))}),i.a.createElement(c.a,{container:!0,justify:"center",style:{marginTop:50,textAlign:"center"}},i.a.createElement(c.a,{item:!0,xs:3},i.a.createElement(v.a,{style:{color:"white",backgroundColor:"#4885ed",textAlign:"center"}},i.a.createElement("a",{href:W.a,rel:"noopener noreferrer",target:"_blank",style:{color:"white",textDecoration:"none"}},"Download PDF Version")))),i.a.createElement(J,null))}}]),t}(i.a.Component),U=Object(A.d)(H),G=a(93),Z=a.n(G),_=a(94),$=a.n(_),ee=a(95),te=a.n(ee),ae=a(47),ne=a.n(ae),ie=a(98),re=a.n(ie),oe=a(48),le=a.n(oe),ce=a(97),se=a.n(ce),me=a(96),ue=a.n(me),pe=[[{title:"Deep Reinforcement Learning for MAS",description:"A deep reinforcement learning framework applied to Pursuer/Evader world. The policy to be learned aims on taking a team of agents to the victory by catching a team of evaders.",code:"https://github.com/cortandr/MASRL",img:Z.a,more:"/masdrl",completed:"Completed",c:"#009b7d"},{title:"Deep Reinforcement Learning for Games",description:"Self play DQN algorithm aimed on learning how to play the game Dots and Boxes.",code:"",img:$.a,more:"/dots",completed:"Completed",c:"#009b7d"}],[{title:"Viw",description:"Vi(m) plugin for Atom.",img:ue.a,code:"",more:"",completed:"Completed",c:"#009b7d"},{title:"ImageAI",description:"An application built to perform image analysis using state of the art deep learning techniques.",code:"",img:"",more:"",completed:"In - Progress",c:"#4885ed"}],[{title:"Speed Prediction",description:"Machine Leaning algorithm to predict vehicle speed from dash-cam video.",code:"",img:te.a,more:"",completed:"In - Progress",c:"#4885ed"},{title:"Sentiment Analysis",description:"Deep Learning models applied to Natural Language Processing for sentiment classification of Tweets.",code:"",img:"",more:"",completed:"Completed",c:"#4885ed"}]],de=function(e){function t(){return Object(s.a)(this,t),Object(u.a)(this,Object(p.a)(t).apply(this,arguments))}return Object(g.a)(t,e),Object(m.a)(t,[{key:"render",value:function(){var e=this;return i.a.createElement(c.a,{container:!0,justify:"center"},i.a.createElement(k,null),i.a.createElement(R.a,{in:!0,timeout:1e3},i.a.createElement(c.a,{container:!0,justify:"center",style:{marginTop:80}},i.a.createElement(c.a,{item:!0,xs:12},i.a.createElement("h1",{style:{color:"#4885ed",textAlign:"center"}},"Projects")))),i.a.createElement(c.a,{container:!0,style:{marginRight:100,marginLeft:100}},pe.map(function(t,a){return i.a.createElement(R.a,{in:!0,timeout:2e3*(a+1)},i.a.createElement(c.a,{container:!0,justify:"center",style:{marginTop:20},spacing:32},t.map(function(t,a){return i.a.createElement(R.a,{in:!0,timeout:1500*(a+1)},i.a.createElement(c.a,{item:!0,xs:5},i.a.createElement(ne.a,{style:{height:400}},i.a.createElement("div",{style:{height:350}},i.a.createElement(se.a,{style:{height:180,width:"auto"},image:t.img,title:t.title}),i.a.createElement(le.a,null,i.a.createElement(j.a,{gutterBottom:!0,variant:"h5",component:"h2"},t.title),i.a.createElement(j.a,{gutterBottom:!0,variant:"p",component:"p",style:{textAlign:"right",fontSize:13,marginTop:10,marginBottom:10}},"Status : ",i.a.createElement("span",{style:{color:t.c}},t.completed)),i.a.createElement(j.a,{component:"p"},t.description))),i.a.createElement(re.a,null,i.a.createElement(v.a,{size:"small",color:"primary"},"Source Code"),i.a.createElement(v.a,{size:"small",color:"primary",onClick:function(){return e.props.history.push(t.more)}},"Learn More")))))})))})),i.a.createElement(J,null))}}]),t}(i.a.Component),ge=Object(A.d)(de),he=a(99),fe=a.n(he),ye=(a(226),[{title:"Machine Learning",list:["Neural Networks","Tensorflow","Keras","Sci-Kit Learn","Numpy","Pandas","Deep Learning","Reinforcement Learning","Computer Vision"]},{title:"Software Development",list:["Python","Django","Flask","Celery","JavaScript","React","NodeJS","Express","MongoDB"]},{title:"Soft Skills",list:["Music","Photography","Public Speaking","Tennis","Cycling"]}]),Ee=function(e){function t(){return Object(s.a)(this,t),Object(u.a)(this,Object(p.a)(t).apply(this,arguments))}return Object(g.a)(t,e),Object(m.a)(t,[{key:"render",value:function(){return i.a.createElement(c.a,{container:!0,justify:"center"},i.a.createElement(k,null),i.a.createElement(R.a,{in:!0,timeout:1e3},i.a.createElement(c.a,{container:!0,justify:"center",style:{marginTop:80}},i.a.createElement(c.a,{item:!0,xs:12},i.a.createElement("h1",{style:{color:"#4885ed",textAlign:"center"}},"About Me")))),i.a.createElement(c.a,{container:!0,justify:"center",style:{marginTop:50}},i.a.createElement(c.a,{item:!0,xs:5},i.a.createElement(R.a,{in:!0,timeout:1e3},i.a.createElement("img",{src:fe.a,alt:"about_pic",height:500,width:"auto",style:{borderRadius:"3%"}}))),i.a.createElement(c.a,{item:!0,xs:4},i.a.createElement(R.a,{in:!0,timeout:1e3},i.a.createElement("p",{className:"about"},"Currently pursuing a Master of Science in Artificial Intelligence at KU Leuven, Belgium. I'm an energetic, cross-cultural Electronic Engineer from Polytechnic of Turin. Highly creative and open minded student highly capable of working in teams.")),i.a.createElement(R.a,{in:!0,timeout:1e3},i.a.createElement("p",{className:"about"},"I'm a Software Engineer with a passion for Machine Learning. I strongly believe in human intelligence augmentation through the use of AI.")))),i.a.createElement(c.a,{container:!0,justify:"center",style:{backgroundColor:"#F8F8F8",marginTop:80,height:600}},i.a.createElement(R.a,{in:!0,timeout:1e3},i.a.createElement(c.a,{container:!0,justify:"center",style:{marginTop:10}},i.a.createElement(c.a,{item:!0,xs:12},i.a.createElement("h1",{style:{color:"#4885ed",textAlign:"center"}},"Skills")))),i.a.createElement(c.a,{container:!0,justify:"center",style:{marginTop:10,marginRight:100,marginLeft:100},spacing:32},ye.map(function(e,t){return i.a.createElement(c.a,{item:!0,xs:4},i.a.createElement(R.a,{in:!0,timeout:1e3},i.a.createElement(ne.a,{style:{height:400}},i.a.createElement(le.a,{style:{textAlign:"center"}},i.a.createElement(j.a,{variant:"h5",component:"h2",style:{color:"#4885ed"}},e.title),e.list.map(function(e,t){return i.a.createElement(c.a,{container:!0,justify:"center",style:{marginTop:10}},i.a.createElement(c.a,{item:!0,xs:12},i.a.createElement(X.a,{label:e,style:{textAlign:"center",marginRight:5,height:25,borderColor:"#4885ed",color:"#4885ed",fontWeight:"bold"},variant:"outlined"})))})))))}))),i.a.createElement(J,null))}}]),t}(i.a.Component),be=Object(A.d)(Ee),we=a(100),je=a.n(we),xe=a(101),ve=a.n(xe),Ae=a(102),Te=a.n(Ae),ke=[{name:"Global Capture Reward (GCR)",info:"Difference between numbers of components of each team at time step t.",image:""},{name:"Local Capture Reward (LCR)",info:"Reward based on current agent having captured an opponent at time step t.",image:""},{name:"Reachability Reward (RR)",info:"Reward representing how much of the environment is in control of each of the two teams. Reachability is defined as distance from agent to cell",image:ve.a}],Se=function(e){function t(){return Object(s.a)(this,t),Object(u.a)(this,Object(p.a)(t).apply(this,arguments))}return Object(g.a)(t,e),Object(m.a)(t,[{key:"render",value:function(){return i.a.createElement(c.a,{container:!0,justify:"center"},i.a.createElement(c.a,{container:!0,justify:"center",style:{marginLeft:200,marginRight:200}},i.a.createElement(k,null),i.a.createElement(R.a,{in:!0,timeout:1e3},i.a.createElement(c.a,{container:!0,justify:"center",style:{marginTop:80}},i.a.createElement(c.a,{item:!0,xs:12},i.a.createElement("h1",{style:{color:"#4885ed",textAlign:"center"}},"Deep Reinforcement Learning for Multi Agent Systems")))),i.a.createElement(R.a,{in:!0,timeout:1e3},i.a.createElement(c.a,{container:!0,justify:"center",style:{marginTop:30}},i.a.createElement(c.a,{item:!0,xs:12},i.a.createElement("h1",{style:{color:"#4885ed"}},"Problem Definition")))),i.a.createElement(R.a,{in:!0,timeout:1e3},i.a.createElement(c.a,{container:!0,justify:"center",style:{marginTop:10}},i.a.createElement(c.a,{item:!0,xs:12},i.a.createElement("p",{style:{textAlign:"justify"}},"Reinforcement Learning defines itself as a branch of machine learning trying to exploit the science of decision making. The problem of decision making becomes particularly complex in the case of multi-agent systems (MAS) and the increase in number of available agents in an environment. In this work we attempt of solving the approach to such dimensionality issue by applying deep reinforcement learning (DRL) which uses a function approximator to estimate Q-Values for the set of allowed actions."),i.a.createElement("p",{style:{textAlign:"justify"}},"The problem taken under examination is a Pursuit-Evasion problem. Two separate teams are defined. One team, the evaders, is trying to escape from the other team, the pursuers, trying to catch the evaders. The agents in the problem are heterogeneous. In this exercise we focus on trying to apply the MADRL to learn pursuit policies. The evaders are set to follow a simple heuristic policy which takes them further away from the closest pursuer. Simplifications are applied to the problem buy utilizing a matrix like state representation and by using discrete time and space. The experiment is run on a 10x10 environment board with two teams of 5 elements each.")))),i.a.createElement(R.a,{in:!0,timeout:1e3},i.a.createElement(c.a,{container:!0,justify:"center",style:{marginTop:30}},i.a.createElement(c.a,{item:!0,xs:12},i.a.createElement("h1",{style:{color:"#4885ed"}},"Approach")))),i.a.createElement(R.a,{in:!0,timeout:1e3},i.a.createElement(c.a,{container:!0,justify:"center",style:{marginTop:10}},i.a.createElement(c.a,{item:!0,xs:12},i.a.createElement("p",{style:{textAlign:"justify"}},"As stated above the problem was simplified with a GridWorld-like interpretation. Such choice gave the opportunity of exploiting models Convolutional Neural Networks for such problem."),i.a.createElement("p",{style:{textAlign:"justify"}},"The environment representation is described by the following figure also representing the model's input tensor:")))),i.a.createElement(R.a,{in:!0,timeout:1e3},i.a.createElement(c.a,{container:!0,justify:"center",style:{marginTop:10}},i.a.createElement(c.a,{item:!0},i.a.createElement("img",{src:je.a,alt:"state_pic",height:300,width:"auto",style:{borderRadius:"3%"}})))),i.a.createElement(R.a,{in:!0,timeout:1e3},i.a.createElement(c.a,{container:!0,justify:"center",style:{marginTop:10}},i.a.createElement(c.a,{item:!0,xs:12},i.a.createElement("p",{style:{textAlign:"justify"}},"A key element of a reinforcement learning algorithm is the reward signal generated by the environment for the agents. Such function must be carefully studied and design in order to exploit as much information as possible for the interaction agents-environment For this problem three different components of the function were defined:")))),i.a.createElement(R.a,{in:!0,timeout:1e3},i.a.createElement(c.a,{container:!0,justify:"center",style:{marginTop:10}},ke.map(function(e,t){return i.a.createElement(c.a,{item:!0,xs:4},i.a.createElement("h3",{style:{textAlign:"center"}},e.name),i.a.createElement("p",{style:{textAlign:"center"}},e.info,""!==e.image?i.a.createElement("img",{src:e.image,alt:"state_pic",height:100,width:"auto",style:{marginTop:10,borderRadius:"3%"}}):i.a.createElement("div",null)))}))),i.a.createElement(R.a,{in:!0,timeout:1e3},i.a.createElement(c.a,{container:!0,justify:"center",style:{marginTop:10}},i.a.createElement(c.a,{item:!0,xs:12},i.a.createElement("p",{style:{textAlign:"justify"}},"Rewards are scaled to a range r \u2208 [0, number of agents]. An interpolation between these weights is defined by the following formulas:")))),i.a.createElement(R.a,{in:!0,timeout:1e3},i.a.createElement(c.a,{container:!0,justify:"center",style:{marginTop:10}},i.a.createElement(c.a,{item:!0},i.a.createElement("img",{src:Te.a,alt:"rewards_pic",height:50,width:"auto",style:{marginTop:10,borderRadius:"3%"}})))),i.a.createElement(R.a,{in:!0,timeout:1e3},i.a.createElement(c.a,{container:!0,justify:"center",style:{marginTop:10}},i.a.createElement(c.a,{item:!0,xs:12},i.a.createElement("p",{style:{textAlign:"justify"}},"Given the above specification several training rounds and experiments were run in order to evaluate the chosen approach."))))),i.a.createElement(J,null))}}]),t}(i.a.Component),Pe=Object(A.d)(Se),Ce=a(103),Oe=a.n(Ce),Le=a(104),Re=a.n(Le),De=a(105),Ie=a.n(De),Fe=function(e){function t(){return Object(s.a)(this,t),Object(u.a)(this,Object(p.a)(t).apply(this,arguments))}return Object(g.a)(t,e),Object(m.a)(t,[{key:"render",value:function(){return i.a.createElement(c.a,{container:!0,justify:"center"},i.a.createElement(c.a,{container:!0,justify:"center",style:{marginLeft:200,marginRight:200}},i.a.createElement(k,null),i.a.createElement(R.a,{in:!0,timeout:1e3},i.a.createElement(c.a,{container:!0,justify:"center",style:{marginTop:80}},i.a.createElement(c.a,{item:!0,xs:12},i.a.createElement("h1",{style:{color:"#4885ed",textAlign:"center"}},"Deep Reinforcement Learning for Games")))),i.a.createElement(R.a,{in:!0,timeout:1e3},i.a.createElement(c.a,{container:!0,justify:"center",style:{marginTop:30}},i.a.createElement(c.a,{item:!0,xs:12},i.a.createElement("h1",{style:{color:"#4885ed"}},"Problem Definition")))),i.a.createElement(R.a,{in:!0,timeout:1e3},i.a.createElement(c.a,{container:!0,justify:"center",style:{marginTop:10}},i.a.createElement(c.a,{item:!0,xs:12},i.a.createElement("p",{style:{textAlign:"justify"}},"Deep Reinforcement Learning has proven outstanding potential when applied to games and self-play. This work attempts to replicate such results and study RL applied to a farly simple game, Dots and Boxes. Dots and boxes is a game played by two players. The oard of the game is structure as shown below.")))),i.a.createElement(R.a,{in:!0,timeout:1e3},i.a.createElement(c.a,{container:!0,justify:"center"},i.a.createElement(c.a,{item:!0},i.a.createElement("img",{src:Re.a,alt:"state_pic",height:300,width:"auto",style:{borderRadius:"3%"}})))),i.a.createElement(R.a,{in:!0,timeout:1e3},i.a.createElement(c.a,{container:!0,justify:"center"},i.a.createElement(c.a,{item:!0,xs:12},i.a.createElement("p",{style:{textAlign:"justify"}},"Players take turns by drawing a segment connecting two dots horizontally or vertically. A player scores a point if he/she manages to close a box by drawing its last segment. The proposed approach is able to play different board sizes and has been trained by means of self-play and DQN algorithm."),i.a.createElement("p",{style:{textAlign:"justify"}},'This work was part of a course project @KULeuven for the course "Machine Learning Project". The UI and game environment was provided as part of a skeleton, this performed here is specifically related to the Reinforcement Learning agent learning.')))),i.a.createElement(R.a,{in:!0,timeout:1e3},i.a.createElement(c.a,{container:!0,justify:"center",style:{marginTop:"8px"}},i.a.createElement(c.a,{item:!0},i.a.createElement("img",{src:Oe.a,alt:"state_pic",height:300,width:"auto",style:{border:"0.2px solid #F8F8F9",boxShadow:"2px 2px 2px 3px #F8F8F9"}})))),i.a.createElement(R.a,{in:!0,timeout:1e3},i.a.createElement(c.a,{container:!0,justify:"center",style:{marginTop:30}},i.a.createElement(c.a,{item:!0,xs:12},i.a.createElement("h1",{style:{color:"#4885ed"}},"Approach")))),i.a.createElement(R.a,{in:!0,timeout:1e3},i.a.createElement(c.a,{container:!0,justify:"center",style:{marginTop:10}},i.a.createElement(c.a,{item:!0,xs:12},i.a.createElement("p",{style:{textAlign:"justify"}},"The problem was approached as follows. The board was represented as an image like matrix where each cell would represent the top left corner of each board cell and would hold information regarding two segments connected to it, horizontal to the right and vertical down."),i.a.createElement("p",{style:{textAlign:"justify"}},"Such representation is converted to an input tensor when fed into our model with dimension NxMxZ, where N and M are the board dimensions, while Z is the number feature channels which are divided as follows:"),i.a.createElement("ul",null,i.a.createElement("li",null,"First Channel : Agent segments"),i.a.createElement("li",null,"Second Channel : Opponent segments"),i.a.createElement("li",null,"Third Channel : Action segment")),i.a.createElement("p",{style:{textAlign:"justify"}},"The first two channels represent a board mask holding information on segments owed by each players. While the last channel embeds the action to be evaluated by the model. Hence, the model is run for each available action at time step t. The variable dimension of the board was dealt with by applying a Global average pooling layer after the last Convolutional layer of the network.")))),i.a.createElement(R.a,{in:!0,timeout:1e3},i.a.createElement(c.a,{container:!0,justify:"center",style:{marginTop:10}},i.a.createElement(c.a,{item:!0,xs:12},i.a.createElement("p",{style:{textAlign:"justify"}},"A key element of a reinforcement learning algorithm is the reward signal generated by the environment for the agents. Such function must be carefully studied and design in order to exploit as much information as possible for the interaction agents-environment For this problem a straight forward step reward was chosen. It consists of the score difference between the two players")))),i.a.createElement(R.a,{in:!0,timeout:1e3},i.a.createElement(c.a,{container:!0,justify:"center",style:{marginTop:30}},i.a.createElement(c.a,{item:!0,xs:12},i.a.createElement("h1",{style:{color:"#4885ed"}},"Results")))),i.a.createElement(R.a,{in:!0,timeout:1e3},i.a.createElement(c.a,{container:!0,justify:"center",style:{marginTop:10}},i.a.createElement(c.a,{item:!0,xs:12},i.a.createElement("p",{style:{textAlign:"justify"}},"The resulting agents was tested against benchmark naive agents reporting satisfying results. The two agents are a Random Policy agent and a more naive agent holding a policy on closing boxes when possible and avoiding giving boxes to the opponents.")))),i.a.createElement(R.a,{in:!0,timeout:1e3},i.a.createElement(c.a,{container:!0,justify:"center",style:{marginTop:"8px"}},i.a.createElement(c.a,{item:!0},i.a.createElement("img",{src:Ie.a,alt:"state_pic",height:"auto",width:600}))))),i.a.createElement(J,null))}}]),t}(i.a.Component),Be=Object(A.d)(Fe);var Ne=function(){return i.a.createElement(c.a,{container:!0,justify:"center"},i.a.createElement(M.a,{basename:"portfolio"},i.a.createElement(A.a,{exact:!0,path:"/",component:z}),i.a.createElement(A.a,{path:"/resume",component:U}),i.a.createElement(A.a,{path:"/projects",component:ge}),i.a.createElement(A.a,{path:"/about",component:be}),i.a.createElement(A.a,{path:"/masdrl",component:Pe}),i.a.createElement(A.a,{path:"/dots",component:Be})))};Boolean("localhost"===window.location.hostname||"[::1]"===window.location.hostname||window.location.hostname.match(/^127(?:\.(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)){3}$/));o.a.render(i.a.createElement(Ne,null),document.getElementById("root")),"serviceWorker"in navigator&&navigator.serviceWorker.ready.then(function(e){e.unregister()})},35:function(e,t,a){},81:function(e,t,a){},89:function(e,t,a){e.exports=a.p+"static/media/AC_PHOTO.f819f7c7.jpg"},92:function(e,t,a){e.exports=a.p+"static/media/AndreaCortoniCV.b76a409d.pdf"},93:function(e,t,a){e.exports=a.p+"static/media/masdrl.fc8f54a7.png"},94:function(e,t,a){e.exports=a.p+"static/media/dots.f1d9cdc5.png"},95:function(e,t,a){e.exports=a.p+"static/media/speed.56744508.jpg"},96:function(e,t,a){e.exports=a.p+"static/media/vimatom.0500b75c.png"},99:function(e,t,a){e.exports=a.p+"static/media/about_pic.5c30e5a6.JPG"}},[[106,1,2]]]);
//# sourceMappingURL=main.fa71ae8b.chunk.js.map